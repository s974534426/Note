# ch4 Sequential Problems 完全可观察环境中的概率规划系统

## 0. 背景
经典规划的基本假设
1. 有限系统
2. 完全可观察
3. 确定性，每个行动只会导致一种确定的影响
4. 静态性，环境的改变都来自agent的行动
5. 规划目标，目标是可到达的一些状态
6. 序列规划，规划结果是一个线性行动序列
7. 隐含时间，不考虑时间的连续性
8. 离线规划，规划求解器不考虑执行时的状态

例子有，积木世界，求解方法分为状态空间求解和规划空间求解

这节课讲的主要是概率规划，基于概率模型和效用函数，制定决策

将问题描述为MDP或POMDP

求解方法有离线规划：例如动态规划，在线规划：蒙特卡洛树搜索

书中的4-6章讨论序贯决策问题，第4章为模型已知，模型完全可观察(MDP规划)，第5章为模型未知，环境完全可观察(强化学习)，第6章为模型已知，环境部分可观察(POMDP规划)

## 1. Formulation
这一章的内容关于在模型已知且环境完全可观察时的序贯决策

### 1. Markov Decision Processes
马尔科夫决策过程MDP中，一个agent在时间t基于当前的观察$o_t$选择行动action$_t$，然后接受奖励$r_t$

下一状态仅仅与当前观察和行动，与先前的所有观察和行动无关

### 2. Utility and Reward
介绍了定义奖励的一些方法

## 2， Dynamic Programming
### 1. Policies and Utilities
MDP中的一个policy指的是在给定历史状态和action时，确定做出什么样的行动。在t时刻选择的行动写做$\pi_t(s_{0:t}, a_{0: t-1})$

在状态s下执行$\pi$的期望效用为$U^{\pi}(s)$，在MDP问题中，$U^{\pi}$通常指的是值函数，一个最优策略是最大化效用的策略$\pi^*(s)=argmax_{\pi}U^{\pi}(s)$

### 2. Policy Evaluation
计算一个policy的期望效用称为policy evaluation

计算执行策略$\pi$，t步后的效用

### 7. 闭环规划和开环规划
开环规划不考虑未来状态信息，也就是会提前规划一条路径，然后走，计算开销小，但结果次优

闭环规划会考虑未来信息状态，根据观察到当前的状态来选择行动，计算开销大，能后的近似最优解