# ch4 Sequential Problems 完全可观察环境中的概率规划系统
第四五节好难。。。策略和值函数，有时间重看并记笔记
## 0. 背景
经典规划的基本假设
1. 有限系统
2. 完全可观察
3. 确定性，每个行动只会导致一种确定的影响
4. 静态性，环境的改变都来自agent的行动
5. 规划目标，目标是可到达的一些状态
6. 序列规划，规划结果是一个线性行动序列
7. 隐含时间，不考虑时间的连续性
8. 离线规划，规划求解器不考虑执行时的状态

例子有，积木世界，求解方法分为状态空间求解和规划空间求解

这节课讲的主要是概率规划，基于概率模型和效用函数，制定决策

将问题描述为MDP或POMDP

求解方法有离线规划：例如动态规划，在线规划：蒙特卡洛树搜索

书中的4-6章讨论序贯决策问题，第4章为模型已知，模型完全可观察(MDP规划)，第5章为模型未知，环境完全可观察(强化学习)，第6章为模型已知，环境部分可观察(POMDP规划)

## 1. Formulation
这一章的内容关于在模型已知且环境完全可观察时的序贯决策

### 1. Markov Decision Processes
#### 定义
马尔科夫假设：下一状态仅仅与当前观察和行动，与先前的所有观察和行动无关，$P(S_{t+1}=s', R_t=r|S_t=s, A_t=a)$

马尔科夫决策过程MDP中，一个agent在时间t基于当前的观察$o_t$选择行动action$_t$，然后接受奖励$r_t$

![20200409191853](https://raw.githubusercontent.com/s974534426/Img_for_notes/master/20200409191853.png)

#### 轨道
一个时间离散化的agent环境交互过程可以用轨道来表示$S_0, O_0, A_0, R_0, ..., S_n, O_n, A_n, R_n, ...$

无限步数(连续式)决策任务：交互要一直进行下去

有限步数(情节式，回合制)决策任务有一个结束状态s

完全可观察时，$O_t = S_t$，上面的轨道可以省略观察

#### 稳态MDPs
$P(S_{t+1}, R_t| S_t, A_t)$不随时间变化

![20200409194022](https://raw.githubusercontent.com/s974534426/Img_for_notes/master/20200409194022.png)

![20200409194056](https://raw.githubusercontent.com/s974534426/Img_for_notes/master/20200409194056.png)

### 2. Utility and Reward
介绍了定义奖励的一些方法

MDP中的奖赏可视为一个加法效用函数的组件

#### 有限步数
奖赏为$\sum_{t=0}^{n-1}R_t$

#### 无限步数
- 使用折扣因子定义效用，$\sum_{t=0}^{\infin}\gamma^tR_t$
- 使用平均奖赏定义效用，$lim_{n\rightarrow \infin}\frac{1}{n}\sum_{t=0}^{n-1}R_t$

折扣因子的作用
- 使得当前奖励比未来奖赏更有价值
- 只要奖赏有限，效用也是有限数

#### 状态值函数
![20200409202228](https://raw.githubusercontent.com/s974534426/Img_for_notes/master/20200409202228.png)


## 2， Dynamic Programming
### 1. Policies and Utilities
MDP中的一个policy指的是在给定历史状态和action时，确定做出什么样的行动。在t时刻选择的行动写做$\pi_t(s_{0:t}, a_{0: t-1})$

在状态s下执行$\pi$的期望效用为$U^{\pi}(s)$，在MDP问题中，$U^{\pi}$通常指的是值函数，一个最优策略是最大化效用的策略$\pi^*(s)=argmax_{\pi}U^{\pi}(s)$

### 2. Policy Evaluation
计算一个policy的期望效用称为policy evaluation

计算执行策略$\pi$，t步后的效用

### 7. 闭环规划和开环规划
开环规划不考虑未来状态信息，也就是会提前规划一条路径，然后走，计算开销小，但结果次优

闭环规划会考虑未来信息状态，根据观察到当前的状态来选择行动，计算开销大，能后的近似最优解