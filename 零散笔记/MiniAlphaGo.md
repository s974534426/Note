- 减少搜索空间的方法
    - 减少深度，用估值函数替代子树
    - 减少宽度，rollout

- 将棋盘19*19的图片传到CNN中，使用价值网络评估棋盘位置，使用策略网络选择行动
- 首先训练一个监督学习的策略网络，从人类专家学习；训练一个快速的策略网络用于rollout；再有一个强化学习的网络改进监督学习的策略网络性能；最后训练一个估值网络，用于判断强化学习网络与自身对决时的胜者
- 监督学习的策略网络
- 强化学习的策略网络
    - 与监督学习的网络相同，初始值也相同，但通过当前网络与随机选择先前的一个网络进行对战更新参数（随机选择一个网络可以防止过拟合到当前网络）
    - 参数更新的方法为随机梯度下降
- 强化学习的值网络
  - 网络结构与策略网络相同，但是输出值是一个预测值而不是概率分布，我们通过简单地对状态s和值z的回归训练网络，使用随机梯度下降最小化均方误差
  - 但这样会导致过拟合的问题，论文中通过进行多盘游戏，每个游戏都使用RL的策略网络与自己对抗，产生数据，利用这些独立的数据进行回归，产生了很好的效果
  - 相比rollout，使用值网络更加准确，但计算次数多与rollout的计算次数
- 利用值网络和策略网络进行搜索
  - 每一个叶节点的值通过两种方式估值，值网络和rollout，并进行加权平均$V(s_L)=(1- \lambda)v_\theta (s_L)+\lambda z_L$
  - AlphaGo使用并行计算进行加速
- 评估AlphaGo的性能
  - 在论文中认为上面的加权和中$\lambda=0.5$会达到最好效果，这说明两种估值方式是互补的，值网络更准确但也更慢，rollout更快但不精确